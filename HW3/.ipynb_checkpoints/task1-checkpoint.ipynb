{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from collections import defaultdict, OrderedDict\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def readJson(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return df\n",
    "\n",
    "df = readJson('assignment1/train.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalData = [df[x] for x in range(0,200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# np.random.shuffle(df)\n",
    "trainingData = [df[x] for x in range(0,200000)]\n",
    "validationData = [df[x] for x in range(100000,200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': [['Clothing, Shoes & Jewelry',\n",
       "   'Women',\n",
       "   'Clothing',\n",
       "   'Lingerie, Sleep & Lounge',\n",
       "   'Intimates',\n",
       "   'Bras',\n",
       "   'Everyday Bras']],\n",
       " 'categoryID': 0,\n",
       " 'helpful': {'nHelpful': 0, 'outOf': 0},\n",
       " 'itemID': 'I835860961',\n",
       " 'rating': 4.0,\n",
       " 'reviewHash': 'R723643278',\n",
       " 'reviewText': 'This item fit really well. It did not stretch. It stayed comfortable the whole time I had it on. Would buy another one.',\n",
       " 'reviewTime': '05 2, 2014',\n",
       " 'reviewerID': 'U072718749',\n",
       " 'summary': 'fits well',\n",
       " 'unixReviewTime': 1398988800}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData[12836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredTrainingData = [data for data in trainingData if data['helpful']['outOf'] > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number review\n",
    "globalRating = []\n",
    "userNumReview, itemNumReview = defaultdict(int), defaultdict(int)\n",
    "userRatings = defaultdict(list)\n",
    "itemRatings = defaultdict(list)\n",
    "for data in totalData:\n",
    "    globalRating.append(data['rating'])\n",
    "    user,item = data['reviewerID'], data['itemID']\n",
    "    globalRating.append(data['rating'])\n",
    "    userRatings[user].append(data['rating'])\n",
    "    itemRatings[item].append(data['rating'])\n",
    "    userNumReview[user] += 1\n",
    "    itemNumReview[data['itemID']] += 1\n",
    "    \n",
    "globalAverageRating = np.mean(globalRating)\n",
    "userAverageRating, itemAverageRating = defaultdict(lambda:globalAverageRating), defaultdict(lambda:globalAverageRating)\n",
    "userRatingStd, itemRatingStd = defaultdict(float), defaultdict(float)\n",
    "for u in userRatings:\n",
    "    userAverageRating[u] = np.mean(userRatings[u])\n",
    "    userRatingStd[u] = np.std(userRatings[u])\n",
    "for i in itemRatings:\n",
    "    itemAverageRating[i] = np.mean(itemRatings[i])\n",
    "    itemRatingStd[i] = np.std(itemRatings[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode product type\n",
    "categoryIndex = ['Men', 'Women', 'Boys', 'Girls', 'Baby']\n",
    "numCategory = 5\n",
    "\n",
    "def encodeCategory(data, cat):\n",
    "    if cat == 5:\n",
    "        return 1\n",
    "    for category in data['categories']:\n",
    "        for subcategory in category:\n",
    "            if subcategory == categoryIndex[cat]:\n",
    "                return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categoryDic = defaultdict(int)\n",
    "for data in trainingData:\n",
    "    for category in data['categories']:\n",
    "        categoryDic[category[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFeature(dataset):\n",
    "    return np.array([feature(data) for data in dataset])\n",
    "def feature(data):\n",
    "    return np.array([1, \n",
    "#                      len([w for w in data['reviewText'].split() if w not in stopWords]),\n",
    "#                      len(data['reviewText'].split()),\n",
    "                    len(data['reviewText'].split())*1./data['rating'],\n",
    "                    len(data['reviewText'].split())*itemRatingStd[data['itemID']],\n",
    "#                       [len(tokenize.sent_tokenize(data['reviewText'])) for data in dataset],\n",
    "#                       [np.sum([word.upper() == word for word in data['reviewText'].split()])*1./(len(data['reviewText'].split())+1) for data in dataset],\n",
    "                    data['reviewText'].count('?')*1./len(data['reviewText'].split('.')),\n",
    "                    data['reviewText'].count('!')*1./len(data['reviewText'].split('.')),\n",
    "#                     data['reviewText'].count('but')+data['reviewText'].count('However'),\n",
    "#                       [data['reviewText'].count('but')**3 for data in dataset],\n",
    "#                     data['reviewText'].count('awesome'),\n",
    "#                       ['but' in data['reviewText'].split() or 'However' in data['reviewText'].split() or 'But' in data['reviewText'].split()  for data in dataset],\n",
    "                      \n",
    "#                       [np.mean([wordDic[w] for w in set(nltk.word_tokenize(data['reviewText']))]) for data in dataset],\n",
    "#                       [np.sum(tfidf.transform([data['reviewText']]))*data['rating'] for data in dataset],\n",
    "\n",
    "                      userNumReview[data['reviewerID']],\n",
    "#                       [userNumReview[data['reviewerID']]*1./len(data['reviewText'].split('.')) for data in dataset],\n",
    "#                       [userRatingStd[data['reviewerID']] for data in dataset],\n",
    "#                       [abs(userAverageRating[data['reviewerID']]-data['rating']) for data in dataset],\n",
    "                      itemRatingStd[data['itemID']],\n",
    "                      \n",
    "                      abs(itemAverageRating[data['itemID']]-data['rating']),\n",
    "                      \n",
    "                      encodeCategory(data, 0)+encodeCategory(data, 2),\n",
    "                      encodeCategory(data, 1)+encodeCategory(data, 3),\n",
    "#                       min(encodeCategory(data, 0)+encodeCategory(data, 2),1)-userNumReview[data['reviewerID']],\n",
    "#                       min(encodeCategory(data, 1)+encodeCategory(data, 3),1)-userNumReview[data['reviewerID']],\n",
    "#                       encodeCategory(data, 1), \n",
    "#                      encodeCategory(data, 0),\n",
    "                     encodeCategory(data, 2),\n",
    "#                       encodeCategory(data, 3),\n",
    "#                       encodeCategory(data, 4),\n",
    "#                      encodeCategory(data, 0)*(data['rating']-globalAverageRating),\n",
    "#                      encodeCategory(data, 1)*(data['rating']-globalAverageRating),\n",
    "#                       encodeCategory(data, 2)*(data['rating']-globalAverageRating),\n",
    "#                      encodeCategory(data, 3)*(data['rating']-globalAverageRating),\n",
    "#                      encodeCategory(data, 4)*(data['rating']-globalAverageRating),\n",
    "                     \n",
    "#                      encodeCategory(data, 0)* (data['rating']-itemAverageRating[data['itemID']]),\n",
    "#                      encodeCategory(data, 1)*(data['rating']-itemAverageRating[data['itemID']]),\n",
    "#                       encodeCategory(data, 2)*(data['rating']-itemAverageRating[data['itemID']]),\n",
    "#                      encodeCategory(data, 3)*(data['rating']-itemAverageRating[data['itemID']]),\n",
    "#                      encodeCategory(data, 4)*(data['rating']-itemAverageRating[data['itemID']]),\n",
    "                     \n",
    "#                       data['helpful']['outOf'],\n",
    "                      data['rating'],\n",
    "                      data['rating']-itemAverageRating[data['itemID']],\n",
    "                     \n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nhelpful = np.array([data['helpful']['nHelpful'] for data in filteredTrainingData])\n",
    "train_outOf = np.array([data['helpful']['outOf'] for data in filteredTrainingData])\n",
    "validation_nhelpful = np.array([data['helpful']['nHelpful'] for data in validationData])\n",
    "validation_outOf = np.array([data['helpful']['outOf'] for data in validationData])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = generateFeature(filteredTrainingData)\n",
    "y_train = train_nhelpful*1. / train_outOf\n",
    "theta= np.linalg.lstsq(X_train, y_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18193681057317612"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(np.dot(generateFeature(trainingData), theta)* \n",
    "                    np.array([data['helpful']['outOf'] for data in trainingData]), \n",
    "                    np.array([data['helpful']['nHelpful'] for data in trainingData])) #181434,0.18117460510755362"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17423"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation = generateFeature(validationData)\n",
    "y_validation = validation_nhelpful\n",
    "mean_absolute_error(np.round(np.dot(X_validation, theta)* validation_outOf), y_validation) #173919,0.17423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"assignment1/pairs_Helpful.txt\") as f:\n",
    "    pairsHelpful = f.readlines()\n",
    "\n",
    "reviewDataForTest = readJson('assignment1/test_Helpful.json.gz')\n",
    "predictions = open(\"assignment1/predictions_Helpful.txt\", 'w')\n",
    "i = 0\n",
    "for l in open(\"assignment1/pairs_Helpful.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user ,item,outOf = l.strip().split('-')\n",
    "    outOf = int(outOf)\n",
    "    predictHelpfulRate = min(np.dot(generateFeature([reviewDataForTest[i]]),theta)[0],1)\n",
    "    predictions.write(user + '-' + item + '-' + str(outOf) + ',' + str(np.round(outOf*predictHelpfulRate)) + '\\n')\n",
    "    i += 1\n",
    "    \n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userIndex = {}\n",
    "users = []\n",
    "userToItem = defaultdict(list)\n",
    "itemIndex = {}\n",
    "items = []\n",
    "itemToUser = defaultdict(list)\n",
    "i,j = 0,0\n",
    "for data in trainingData:\n",
    "    userToItem[data['reviewerID']].append(data['itemID'])\n",
    "    itemToUser[data['itemID']].append(data['reviewerID'])\n",
    "    if data['reviewerID'] not in userIndex:\n",
    "        userIndex[data['reviewerID']] = i\n",
    "        users.append(data['reviewerID'])\n",
    "        i += 1\n",
    "    if data['itemID'] not in itemIndex:\n",
    "        itemIndex[data['itemID']] = j\n",
    "        items.append(data['itemID'])\n",
    "        j += 1\n",
    "Rui = np.zeros((len(users), len(items)))\n",
    "for data in trainingData:\n",
    "    Rui[userIndex[data['reviewerID']]][itemIndex[data['itemID']]] = data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "globalRating = []\n",
    "userRatings = defaultdict(list)\n",
    "itemRatings = defaultdict(list)\n",
    "for data in trainingData:\n",
    "    user,item = data['reviewerID'], data['itemID']\n",
    "    globalRating.append(data['rating'])\n",
    "    userRatings[user].append(data['rating'])\n",
    "    itemRatings[item].append(data['rating'])\n",
    "\n",
    "globalAverageRating = np.mean(globalRating)\n",
    "userAverageRating, itemAverageRating = {}, {}\n",
    "for u in userRatings:\n",
    "    userAverageRating[u] = np.mean(userRatings[u])\n",
    "for i in itemRatings:\n",
    "    itemAverageRating[i] = np.mean(itemRatings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80162046909090046"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationPredictions = []\n",
    "validationRatings = []\n",
    "for data in validationData:\n",
    "    u,i = data['reviewerID'], data['itemID']\n",
    "    prediction = []\n",
    "    if u in userAverageRating:\n",
    "        prediction.append(userAverageRating[u])      \n",
    "    if i in itemAverageRating:\n",
    "        prediction.append(itemAverageRating[i])\n",
    "    elif u not in userAverageRating:\n",
    "        prediction.append(globalAverageRating)\n",
    "    validationPredictions.append(np.mean(prediction))\n",
    "    validationRatings.append(data['rating'])\n",
    "mean_squared_error(validationPredictions, validationRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeAverageRatings():\n",
    "    predictions = open(\"assignment1/predictions_Rating.txt\", 'w')\n",
    "    for l in open(\"assignment1/pairs_Rating.txt\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            #header\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,i = l.strip().split('-')\n",
    "        prediction = []\n",
    "        if u in userAverageRating:\n",
    "            prediction.append(userAverageRating[u])      \n",
    "        if i in itemAverageRating:\n",
    "            prediction.append(itemAverageRating[i])\n",
    "        elif u not in userAverageRating:\n",
    "            prediction.append(globalAverageRating)\n",
    "        predictions.write(u + '-' + i + ',' + str(np.mean(prediction)) + '\\n')\n",
    "\n",
    "    predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(regularization = 1):\n",
    "    alpha = random.random()\n",
    "    np.random.seed(0)\n",
    "    beta_u = np.random.rand(len(userIndex.keys()))-0.5\n",
    "    beta_i = np.random.rand(len(itemIndex.keys()))-0.5\n",
    "#     beta_u = np.ones(len(userIndex.keys()))\n",
    "#     beta_i = np.ones(len(itemIndex.keys()))\n",
    "    maxIter = 1e5\n",
    "    numIter = 0\n",
    "    alpha_path = []\n",
    "    beta_u_path = []\n",
    "    beta_i_path = []\n",
    "    trainingMSE_path = []\n",
    "    validationMSE_path = []\n",
    "    while numIter < maxIter:\n",
    "        alpha = np.sum([data['rating'] - beta_u[userIndex[data['reviewerID']]] - \\\n",
    "                        beta_i[itemIndex[data['itemID']]] for data in trainingData]) / len(trainingData)\n",
    "        beta_u = np.array([np.sum([Rui[userIndex[user]][itemIndex[item]] - alpha - \\\n",
    "                                   beta_i[itemIndex[item]] for item in userToItem[user]])\n",
    "                  /(regularization + len(userToItem[user])) for user in users])\n",
    "        beta_i = np.array([np.sum([Rui[userIndex[user]][itemIndex[item]] - alpha - \\\n",
    "                                   beta_u[userIndex[user]] for user in itemToUser[item]])\n",
    "                  /(regularization + len(itemToUser[item])) for item in items])\n",
    "\n",
    "        trainingMSE = mean_squared_error([alpha + beta_u[userIndex[data['reviewerID']]] + \\\n",
    "                        beta_i[itemIndex[data['itemID']]] for data in trainingData], \\\n",
    "                                        [data['rating'] for data in trainingData])\n",
    "        # Calculate validation MSE\n",
    "#         prediction = np.zeros(len(validationData)) + alpha\n",
    "#         i = 0\n",
    "#         for data in validationData:\n",
    "#             if data['reviewerID'] in userIndex:\n",
    "#                 prediction[i] += beta_u[userIndex[data['reviewerID']]]\n",
    "#             if data['itemID'] in itemIndex:\n",
    "#                 prediction[i] += beta_i[itemIndex[data['itemID']]]\n",
    "#             i += 1\n",
    "#         validationMSE = mean_squared_error(prediction, [data['rating'] for data in validationData])\n",
    "        validationMSE = trainingMSE + regularization*(np.sum(beta_u**2) + np.sum(beta_i**2))/len(trainingData)\n",
    "        \n",
    "        numIter += 1 \n",
    "        trainingMSE_path.append(trainingMSE)\n",
    "        validationMSE_path.append(validationMSE)\n",
    "        alpha_path.append(alpha)\n",
    "        beta_u_path.append(beta_u)\n",
    "        beta_i_path.append(beta_i)\n",
    "        if numIter % 20 == 0:\n",
    "            print trainingMSE, validationMSE\n",
    "        if len(validationMSE_path) >= 6 and validationMSE_path[-5] <= validationMSE_path[-4] <= \\\n",
    "            validationMSE_path[-3] <= validationMSE_path[-2] <= validationMSE_path[-1]:\n",
    "            print 'Iteration:', numIter, 'Training MSE =', trainingMSE_path[-5], \\\n",
    "                  ', Validation MSE =', validationMSE_path[-5]\n",
    "                \n",
    "            return alpha_path[-5], beta_u_path[-5], beta_i_path[-5], trainingMSE_path[-5], validationMSE_path[-5]\n",
    "\n",
    "    return alpha, beta_u, beta_i, trainingMSE, validationMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835956965996 0.835957759464\n",
      "Iteration: 39 Training MSE = 0.835957186137 , Validation MSE = 0.835957186314\n"
     ]
    }
   ],
   "source": [
    "alpha, beta_u, beta_i, trainingMSE, validationMSE = gradientDescent(regularization = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794384391056 0.815945845588\n",
      "0.794384944643 0.794384948844\n",
      "Iteration: 50 Training MSE = 0.794384945656 , Validation MSE = 0.794384945728\n"
     ]
    }
   ],
   "source": [
    "alpha, beta_u, beta_i, trainingMSE, validationMSE = gradientDescent(regularization = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80782431117 0.931452393799\n",
      "0.807824305534 0.931452393767\n",
      "Iteration: 44 Training MSE = 0.807824305534 , Validation MSE = 0.931452393767\n"
     ]
    }
   ],
   "source": [
    "alpha, beta_u, beta_i, trainingMSE, validationMSE = gradientDescent(regularization = 5.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda:  6\n",
      "0.813835489755 1.08963681369\n",
      "Iteration: 23 Training MSE = 0.813835353486 , Validation MSE = 1.08963677674\n"
     ]
    }
   ],
   "source": [
    "regularizationPath = [6]\n",
    "trainingMSE_r_path = []\n",
    "validationMSE_r_path = []\n",
    "for regularization in regularizationPath:\n",
    "    print \"lambda: \",regularization\n",
    "    alpha, beta_u, beta_i,train_MSE,vali_MSE = gradientDescent(regularization)\n",
    "    trainingMSE_r_path.append(train_MSE)\n",
    "    validationMSE_r_path.append(vali_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = open(\"assignment1/predictions_Rating.txt\", 'w')\n",
    "for l in open(\"assignment1/pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    this_beta_u, this_beta_i = 0,0\n",
    "    if u in userIndex:\n",
    "        this_beta_u = beta_u[userIndex[u]]\n",
    "    if i in itemIndex:\n",
    "        this_beta_i = beta_i[itemIndex[i]]\n",
    "        \n",
    "    predictions.write(u + '-' + i + ',' + str(alpha + this_beta_u + this_beta_i) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradientDescent2(regularization = 1, iniStepSize = 0.01, T = 10, k = 3):\n",
    "    alpha = random.random()\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    beta_u = np.random.rand(len(userIndex.keys()))\n",
    "    beta_i = np.random.rand(len(itemIndex.keys()))\n",
    "    r_u = np.random.rand(len(userIndex.keys()))\n",
    "    r_i = np.random.rand(len(itemIndex.keys()))\n",
    "#     beta_u = np.ones(len(userIndex.keys()))\n",
    "#     beta_i = np.ones(len(itemIndex.keys()))\n",
    "    maxIter = 1e3\n",
    "    numIter = 0\n",
    "    alpha_path = []\n",
    "    beta_u_path = []\n",
    "    beta_i_path = []\n",
    "    r_u_path = []\n",
    "    r_i_path = []\n",
    "    trainingMSE_path = []\n",
    "    validationMSE_path = []\n",
    "    while numIter < maxIter:\n",
    "        print numIter\n",
    "        stepSize = iniStepSize /(1+numIter*1./T)\n",
    "        \n",
    "        alpha = alpha - stepSize*(2*np.sum([alpha + beta_u[userIndex[data['reviewerID']]] + \\\n",
    "                                            beta_i[itemIndex[data['itemID']]] + \\\n",
    "                                            np.dot(r_u[userIndex[data['reviewerID']]], r_i[itemIndex[data['itemID']]].T)\\\n",
    "                                            - data['rating'] \\\n",
    "                                           for data in trainingData]) / len(trainingData) + 2*regularization*alpha)\n",
    "        \n",
    "        beta_u = beta_u - stepSize*(2*np.array([np.mean([alpha + beta_u[userIndex[user]] + beta_i[itemIndex[item]] + \\\n",
    "                                                        np.dot(r_u[userIndex[user]], r_i[itemIndex[item]].T) - \\\n",
    "                                                        Rui[userIndex[user]][itemIndex[item]] for item in userToItem[user]])\\\n",
    "                                                for user in users])  + 2*regularization*beta_u)\n",
    "\n",
    "        beta_i = beta_i - stepSize*(2*np.array([np.mean([alpha + beta_u[userIndex[user]] + beta_i[itemIndex[item]] + \\\n",
    "                                                        np.dot(r_u[userIndex[user]], r_i[itemIndex[item]].T) - \\\n",
    "                                                        Rui[userIndex[user]][itemIndex[item]] for user in itemToUser[item]]) \\\n",
    "                                                for item in items])  + 2*regularization*beta_i)\n",
    "        \n",
    "        r_u = r_u - stepSize*(np.array([np.mean([2*r_i[itemIndex[item]]*\\\n",
    "                                                (alpha+beta_u[userIndex[user]]+beta_i[itemIndex[item]]+\\\n",
    "                                                 np.dot(r_u[userIndex[user]], r_i[itemIndex[item]].T)-\\\n",
    "                                                 Rui[userIndex[user]][itemIndex[item]]) \\\n",
    "                                                for item in userToItem[user]], axis = 0) \\\n",
    "                                        for user in users])\\\n",
    "                              +2*regularization*r_u)\n",
    "        \n",
    "        r_i = r_i - stepSize*(np.array([np.mean([2*r_u[userIndex[user]]*\\\n",
    "                                                (alpha+beta_u[userIndex[user]]+beta_i[itemIndex[item]]+\\\n",
    "                                                 np.dot(r_u[userIndex[user]], r_i[itemIndex[item]].T)-\\\n",
    "                                                 Rui[userIndex[user]][itemIndex[item]]) \\\n",
    "                                                for user in itemToUser[item]], axis = 0) \\\n",
    "                                        for item in items])\\\n",
    "                              +2*regularization*r_i)\n",
    "        \n",
    "        trainingMSE = mean_squared_error([alpha + beta_u[userIndex[data['reviewerID']]] + \\\n",
    "                        beta_i[itemIndex[data['itemID']]] + \\\n",
    "                               np.dot(r_u[userIndex[data['reviewerID']]], r_i[itemIndex[data['itemID']]].T) for data in trainingData], \\\n",
    "                                        [data['rating'] for data in trainingData])\n",
    "        # Calculate validation MSE\n",
    "        prediction = np.zeros(len(validationData)) + alpha\n",
    "        i = 0\n",
    "        for data in validationData:\n",
    "            if data['reviewerID'] in userIndex:\n",
    "                prediction[i] += beta_u[userIndex[data['reviewerID']]]\n",
    "            else:\n",
    "                prediction[i] += np.mean(beta_u)\n",
    "            if data['itemID'] in itemIndex:\n",
    "                prediction[i] += beta_i[itemIndex[data['itemID']]]\n",
    "            else:\n",
    "                prediction[i] += np.mean(beta_i)\n",
    "            if data['reviewerID'] in userIndex and data['itemID'] in itemIndex:\n",
    "                prediction[i] += np.dot(r_u[userIndex[data['reviewerID']]], r_i[itemIndex[data['itemID']]].T)\n",
    "            i += 1\n",
    "        validationMSE = mean_squared_error(prediction, [data['rating'] for data in validationData])\n",
    "        print trainingMSE,validationMSE\n",
    "        numIter += 1 \n",
    "        trainingMSE_path.append(trainingMSE)\n",
    "        validationMSE_path.append(validationMSE)\n",
    "        alpha_path.append(alpha)\n",
    "        beta_u_path.append(beta_u)\n",
    "        beta_i_path.append(beta_i)\n",
    "        r_u_path.append(r_u)\n",
    "        r_i_path.append(r_i)\n",
    "        if len(validationMSE_path) >= 6 and validationMSE_path[-5] <= validationMSE_path[-4] <= \\\n",
    "            validationMSE_path[-3] <= validationMSE_path[-2] <= validationMSE_path[-1]:\n",
    "            print 'Iteration:', numIter, 'Training MSE =', trainingMSE_path[-5], \\\n",
    "                  ', Validation MSE =', validationMSE_path[-5]\n",
    "                \n",
    "            return alpha_path[-5], beta_u_path[-5], beta_i_path[-5], r_u_path[-5], r_i_path[-5], trainingMSE_path[-5], validationMSE_path[-5]\n",
    "\n",
    "    return alpha, beta_u, beta_i, trainingMSE, validationMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.10292566718 2.2873201331\n",
      "1\n",
      "1.3969713209 1.85719532025\n",
      "2\n",
      "1.21540313436 2.00465676792\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-41f18aab77f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientDescent2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miniStepSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-b8cc86a619cd>\u001b[0m in \u001b[0;36mgradientDescent2\u001b[0;34m(regularization, iniStepSize, T, k)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbeta_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_u\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m                                                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m                                                         \u001b[0mRui\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserToItem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                                                \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeta_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mbeta_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m                                                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m                                                         \u001b[0mRui\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitemToUser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                                                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeta_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mr_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_u\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m                                                \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbeta_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbeta_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m                                                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m                                                 \u001b[0mRui\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitemIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                                                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserToItem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m                                         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                              \u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liuxiyun/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m             \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2884\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha, beta_u, beta_i, r_u, r_i, trainingMSE, validationMSE = \\\n",
    "gradientDescent2(regularization=1, iniStepSize=0.5, T=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userIndex = {}\n",
    "users = []\n",
    "userToItem = defaultdict(list)\n",
    "itemIndex = {}\n",
    "items = []\n",
    "itemToUser = defaultdict(list)\n",
    "i,j = 0,0\n",
    "lines = []\n",
    "for data in trainingData:\n",
    "    userToItem[data['reviewerID']].append(data['itemID'])\n",
    "    itemToUser[data['itemID']].append(data['reviewerID'])\n",
    "    if data['reviewerID'] not in userIndex:\n",
    "        userIndex[data['reviewerID']] = i\n",
    "        users.append(data['reviewerID'])\n",
    "        i += 1\n",
    "    if data['itemID'] not in itemIndex:\n",
    "        itemIndex[data['itemID']] = j\n",
    "        items.append(data['itemID'])\n",
    "        j += 1\n",
    "    lines.append(str(userIndex[data['reviewerID']])+','+str(itemIndex[data['itemID']])+','+str(data['rating']))\n",
    "\n",
    "for line in open(\"assignment1/pairs_Rating.txt\"):\n",
    "    if line.startswith(\"userID\"):\n",
    "        continue\n",
    "    user,item = line.strip().split('-')\n",
    "    if user not in userIndex:\n",
    "        userIndex[user] = i\n",
    "        i += 1\n",
    "    if item not in itemIndex:\n",
    "        itemIndex[item] = j\n",
    "    lines.append(str(userIndex[user])+','+ str(itemIndex[item]))\n",
    "Rui = np.zeros((len(users), len(items)))\n",
    "for data in trainingData:\n",
    "    Rui[userIndex[data['reviewerID']]][itemIndex[data['itemID']]] = data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "print tfidf.fit_transform(['hello world','hehe world'])\n",
    "print tfidf.get_feature_names()\n",
    "print tfidf.transform(['hello world'])\n",
    "print\n",
    "print tfidf.transform(['hello world'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tfidf.transform(['hello world'])):\n",
    "    \n",
    "wordWeight = [(tfidf.vocabulary_[i], ) for (_, i) in tfidf.transform(['hello world']):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2],[3,4]]\n",
    "np.mean(a, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
