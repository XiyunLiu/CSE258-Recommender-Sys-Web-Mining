{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from collections import defaultdict, OrderedDict\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def readJson(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return df\n",
    "\n",
    "df = readJson('assignment1/train.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalData = [df[x] for x in range(0,200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# np.random.shuffle(df)\n",
    "trainingData = [df[x] for x in range(0,100000)]\n",
    "validationData = [df[x] for x in range(100000,200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': [['Clothing, Shoes & Jewelry',\n",
       "   'Women',\n",
       "   'Clothing',\n",
       "   'Lingerie, Sleep & Lounge',\n",
       "   'Intimates',\n",
       "   'Bras',\n",
       "   'Everyday Bras']],\n",
       " 'categoryID': 0,\n",
       " 'helpful': {'nHelpful': 0, 'outOf': 0},\n",
       " 'itemID': 'I835860961',\n",
       " 'rating': 4.0,\n",
       " 'reviewHash': 'R723643278',\n",
       " 'reviewText': 'This item fit really well. It did not stretch. It stayed comfortable the whole time I had it on. Would buy another one.',\n",
       " 'reviewTime': '05 2, 2014',\n",
       " 'reviewerID': 'U072718749',\n",
       " 'summary': 'fits well',\n",
       " 'unixReviewTime': 1398988800}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData[12836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredTrainingData = [data for data in trainingData if data['helpful']['outOf'] > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number review\n",
    "globalRating = []\n",
    "userNumReview, itemNumReview = defaultdict(int), defaultdict(int)\n",
    "userRatings = defaultdict(list)\n",
    "itemRatings = defaultdict(list)\n",
    "for data in totalData:\n",
    "    globalRating.append(data['rating'])\n",
    "    user,item = data['reviewerID'], data['itemID']\n",
    "    globalRating.append(data['rating'])\n",
    "    userRatings[user].append(data['rating'])\n",
    "    itemRatings[item].append(data['rating'])\n",
    "    userNumReview[user] += 1\n",
    "    itemNumReview[data['itemID']] += 1\n",
    "    \n",
    "globalAverageRating = np.mean(globalRating)\n",
    "userAverageRating, itemAverageRating = defaultdict(lambda:globalAverageRating), defaultdict(lambda:globalAverageRating)\n",
    "userRatingStd, itemRatingStd = defaultdict(float), defaultdict(float)\n",
    "for u in userRatings:\n",
    "    userAverageRating[u] = np.mean(userRatings[u])\n",
    "    userRatingStd[u] = np.std(userRatings[u])\n",
    "for i in itemRatings:\n",
    "    itemAverageRating[i] = np.mean(itemRatings[i])\n",
    "    itemRatingStd[i] = np.std(itemRatings[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode product type\n",
    "categoryIndex = ['Men', 'Women', 'Boys', 'Girls', 'Baby']\n",
    "numCategory = 5\n",
    "\n",
    "def encodeCategory(data, cat):\n",
    "    if cat == 5:\n",
    "        return 1\n",
    "    for category in data['categories']:\n",
    "        for subcategory in category:\n",
    "            if subcategory == categoryIndex[cat]:\n",
    "                return 1\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categoryDic = defaultdict(int)\n",
    "for data in trainingData:\n",
    "    for category in data['categories']:\n",
    "        categoryDic[category[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "wordWeightMatrix = tfidf.fit_transform([data['reviewText'] for data in filteredTrainingData])\n",
    "\n",
    "wordDic = defaultdict(float)\n",
    "wordApperence = defaultdict(int)\n",
    "for data in filteredTrainingData:\n",
    "    for w in nltk.word_tokenize(data['reviewText']):\n",
    "        if w.lower() in stopWords or w in punctuation:\n",
    "            continue\n",
    "            n = wordApperence[w.lower()]\n",
    "            wordDic[w.lower()] = (n * wordDic[w.lower()] + data['helpful']['nHelpful'] * 1.\n",
    "                                  /data['helpful']['outOf'])/(n+1)\n",
    "            wordApperence[w.lower()] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5895, 5350, 5347, 5336, 5329, 5089, 4839, 4136, 3675, 3624, 3454,\n",
       "       3201, 3090, 3015, 2000, 1991, 1452,  904,  718,  479], dtype=int32)"
      ]
     },
     "execution_count": 1497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse = tfidf.transform([trainingData[0]['reviewText']])\n",
    "sparse.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFeature1(dataset):\n",
    "    return np.array([feature1(data) for data in dataset])\n",
    "def feature1(data):\n",
    "    return np.array([1, \n",
    "#                      len([w for w in data['reviewText'].split() if w not in stopWords]),\n",
    "                    len(data['reviewText'].split()),\n",
    "                    len(data['reviewText'].split())*itemRatingStd[data['itemID']],\n",
    "#                       [len(tokenize.sent_tokenize(data['reviewText'])) for data in dataset],\n",
    "#                     np.sum([word.upper() == word for word in data['reviewText'].split()])*1./(len(data['reviewText'].split())+1),\n",
    "                    data['reviewText'].count('?')*1./len(data['reviewText'].split('.')),\n",
    "#                     data['reviewText'].count('!')*1./len(data['reviewText'].split('.')),\n",
    "#                     (data['reviewText'].count('But')+data['reviewText'].count('however'))*1./len(data['reviewText'].split('.')),\n",
    "#                       data['reviewText'].count('but')**3,\n",
    "                      \n",
    "#                       [np.mean([wordDic[w] for w in set(nltk.word_tokenize(data['reviewText']))]) for data in dataset],\n",
    "#                       [np.sum(tfidf.transform([data['reviewText']]))*data['rating'] for data in dataset],\n",
    "\n",
    "                      userNumReview[data['reviewerID']],\n",
    "                      userNumReview[data['reviewerID']]**2,\n",
    "#                       [userNumReview[data['reviewerID']]*1./len(data['reviewText'].split('.')) for data in dataset],\n",
    "#                       [userRatingStd[data['reviewerID']] for data in dataset],\n",
    "#                       [abs(userAverageRating[data['reviewerID']]-data['rating']) for data in dataset],\n",
    "                      itemRatingStd[data['itemID']],\n",
    "                      itemRatingStd[data['itemID']]**2,\n",
    "                      \n",
    "                      abs(itemAverageRating[data['itemID']]-data['rating']),\n",
    "                                           \n",
    "                      np.log(data['helpful']['outOf']+1),\n",
    "                      data['rating'],\n",
    "                      data['rating']-itemAverageRating[data['itemID']],\n",
    "                     \n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFeature2(dataset):\n",
    "    return np.array([feature2(data) for data in dataset])\n",
    "def feature2(data):\n",
    "    return np.array([1, \n",
    "#                      len([w for w in data['reviewText'].split() if w not in stopWords]),\n",
    "#                      len(data['reviewText'].split()),\n",
    "                    len(data['reviewText'].split())*1./data['rating'],\n",
    "                    len(data['reviewText'].split())*itemRatingStd[data['itemID']],\n",
    "#                       [len(tokenize.sent_tokenize(data['reviewText'])) for data in dataset],\n",
    "#                       [np.sum([word.upper() == word for word in data['reviewText'].split()])*1./(len(data['reviewText'].split())+1) for data in dataset],\n",
    "                    data['reviewText'].count('?')*1./len(data['reviewText'].split('.')),\n",
    "                    data['reviewText'].count('!')*1./len(data['reviewText'].split('.')),\n",
    "#                     data['reviewText'].count('but')+data['reviewText'].count('However'),\n",
    "#                       [data['reviewText'].count('but')**3 for data in dataset],\n",
    "#                     data['reviewText'].count('awesome'),\n",
    "#                       ['but' in data['reviewText'].split() or 'However' in data['reviewText'].split() or 'But' in data['reviewText'].split()  for data in dataset],\n",
    "                      \n",
    "#                       [np.mean([wordDic[w] for w in set(nltk.word_tokenize(data['reviewText']))]) for data in dataset],\n",
    "#                       [np.sum(tfidf.transform([data['reviewText']]))*data['rating'] for data in dataset],\n",
    "\n",
    "                      userNumReview[data['reviewerID']],\n",
    "#                       [userNumReview[data['reviewerID']]*1./len(data['reviewText'].split('.')) for data in dataset],\n",
    "#                       [userRatingStd[data['reviewerID']] for data in dataset],\n",
    "#                       [abs(userAverageRating[data['reviewerID']]-data['rating']) for data in dataset],\n",
    "                      itemRatingStd[data['itemID']],\n",
    "                      \n",
    "                      abs(itemAverageRating[data['itemID']]-data['rating']),\n",
    "                      \n",
    "                      encodeCategory(data, 0)+encodeCategory(data, 2),\n",
    "                      encodeCategory(data, 1)+encodeCategory(data, 3),\n",
    "#                       encodeCategory(data, 1), \n",
    "#                      encodeCategory(data, 0),\n",
    "                      encodeCategory(data, 2),\n",
    "#                       encodeCategory(data, 3),\n",
    "#                       encodeCategory(data, 4),\n",
    "                     \n",
    "#                       np.log(data['helpful']['outOf']+1),\n",
    "                      data['rating'],\n",
    "                      data['rating']-itemAverageRating[data['itemID']],\n",
    "                     \n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.592040093342\n",
      "0.522465272234\n"
     ]
    }
   ],
   "source": [
    "filteredTrainingData = [data for data in trainingData if 0<data['helpful']['outOf'] <= 40]\n",
    "train_nhelpful = np.array([data['helpful']['nHelpful'] for data in filteredTrainingData])\n",
    "train_outOf = np.array([data['helpful']['outOf'] for data in filteredTrainingData])\n",
    "filteredValidationData = [data for data in validationData if 0<data['helpful']['outOf'] <= 40]\n",
    "validation_nhelpful = np.array([data['helpful']['nHelpful'] for data in filteredValidationData])\n",
    "validation_outOf = np.array([data['helpful']['outOf'] for data in filteredValidationData])\n",
    "\n",
    "X_train = generateFeature1(filteredTrainingData)\n",
    "y_train = train_nhelpful*1. / train_outOf\n",
    "theta1= np.linalg.lstsq(X_train, y_train)[0]\n",
    "\n",
    "print mean_absolute_error(np.dot(X_train, theta1)* train_outOf, train_nhelpful) #1812\n",
    "\n",
    "X_validation = generateFeature1(filteredValidationData)\n",
    "y_validation = validation_nhelpful\n",
    "\n",
    "print mean_absolute_error(np.round(np.dot(X_validation, theta1)* validation_outOf), y_validation) #173919，173859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.592040093342 0.522465272234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1946441567\n",
      "4.3779264214\n"
     ]
    }
   ],
   "source": [
    "filteredTrainingData = [data for data in trainingData if 40<data['helpful']['outOf']]\n",
    "train_nhelpful = np.array([data['helpful']['nHelpful'] for data in filteredTrainingData])\n",
    "train_outOf = np.array([data['helpful']['outOf'] for data in filteredTrainingData])\n",
    "filteredValidationData = [data for data in validationData if 40<data['helpful']['outOf']]\n",
    "validation_nhelpful = np.array([data['helpful']['nHelpful'] for data in filteredValidationData])\n",
    "validation_outOf = np.array([data['helpful']['outOf'] for data in filteredValidationData])\n",
    "\n",
    "X_train = generateFeature2(filteredTrainingData)\n",
    "y_train = train_nhelpful*1. / train_outOf\n",
    "theta2= np.linalg.lstsq(X_train, y_train)[0]\n",
    "\n",
    "print mean_absolute_error(np.dot(X_train, theta2)* train_outOf, train_nhelpful) #1812\n",
    "\n",
    "X_validation = generateFeature2(filteredValidationData)\n",
    "y_validation = validation_nhelpful\n",
    "print mean_absolute_error(np.round(np.dot(X_validation, theta2)* validation_outOf), y_validation) #173919，173859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5.10765738084 4.11371237458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17732000000000001"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total\n",
    "prediction = []\n",
    "for data in validationData:\n",
    "#     if data['helpful']['outOf'] < 40:\n",
    "#         prediction.append(np.round(min(np.dot(generateFeature1([data]), theta1),1)*data['helpful']['outOf']))\n",
    "#     else:\n",
    "        prediction.append(np.round(min(np.dot(generateFeature2([data]), theta2),1)*data['helpful']['outOf']))\n",
    "mean_absolute_error(prediction, [data['helpful']['nHelpful'] for data in validationData])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.72174973e-02,   1.12026567e-03,  -5.77031412e-04,\n",
       "        -3.50877633e-01,  -2.65969065e-03,   2.83600303e-05,\n",
       "        -8.22997463e-03,  -1.51821326e-02,   1.71410470e-01,\n",
       "         2.22056723e-01,   2.43311639e-01,   2.29602706e-01,\n",
       "         2.66439373e-01,  -9.20163006e-05,   1.57704340e-01,\n",
       "        -1.26866984e-01,  -3.69493069e-02,   8.90677759e-02,\n",
       "         2.71401482e-02,   4.69675009e-03,  -1.67378700e-02])"
      ]
     },
     "execution_count": 1454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"assignment1/pairs_Helpful.txt\") as f:\n",
    "    pairsHelpful = f.readlines()\n",
    "\n",
    "reviewDataForTest = readJson('assignment1/test_Helpful.json.gz')\n",
    "predictions = open(\"assignment1/predictions_Helpful.txt\", 'w')\n",
    "i = 0\n",
    "for l in open(\"assignment1/pairs_Helpful.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user ,item,outOf = l.strip().split('-')\n",
    "    outOf = int(outOf)\n",
    "    predictHelpfulRate = min(np.dot(generateFeature2([reviewDataForTest[i]]),theta2)[0], 1)\n",
    "    predictions.write(user + '-' + item + '-' + str(outOf) + ',' + str(np.round(outOf*predictHelpfulRate)) + '\\n')\n",
    "    i += 1\n",
    "    \n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "def roundi(d,round_bound):\n",
    "    di = np.floor(d)\n",
    "    dr=d-di\n",
    "    if dr>=round_bound:\n",
    "        return di+1\n",
    "    else:\n",
    "        return di\n",
    "\n",
    "#################################################################\n",
    "print 'prepare data'\n",
    "train=pd.read_csv('/Users/limeng/Documents/workspace/CSE_255/Assignment1/train1.csv')\n",
    "training=train[['rating','nhelpful','outof','itemID','reviewerID','reviewText','summary']].head(1000000)\n",
    "training1=training[train.outof>0].set_index(['itemID','reviewerID'],drop=False)\n",
    "training1['alpha']=training1.nhelpful*1.0/training1.outof\n",
    "training1=training1.drop(('I564862346','U684553080'))\n",
    "training1=training1.drop(('I556540507','U179310426'))\n",
    "training1['reviewlen']=training1.reviewText.apply(lambda x:np.log(len(str(x).split())))\n",
    "training1['reviewlen']=training1.reviewText.apply(lambda x:len(str(x).split()))\n",
    "training['reviewlen']=training.reviewText.apply(lambda x:len(str(x).split()))\n",
    "user1=training.groupby('reviewerID')\n",
    "user=training1.groupby('reviewerID')\n",
    "item=training1.groupby('itemID')\n",
    "userinfo=user['alpha'].agg(['mean','std','count','median'])\n",
    "#userinfo=user['alpha'].agg('describe')\n",
    "userinfo=userinfo.fillna(0)\n",
    "userinfo1=user['outof'].agg(['median','std','count'])\n",
    "userinfo1=userinfo1.fillna(0)\n",
    "userinfo2=user['rating'].agg(['median','std'])\n",
    "userinfo2=userinfo2.fillna(0)\n",
    "userinfo3=user['reviewlen'].agg(['median','std'])\n",
    "userinfo3=userinfo3.fillna(0)\n",
    "\n",
    "iteminfo=item['alpha'].agg(['median','mean'])\n",
    "bu=pd.read_csv('/Users/limeng/Desktop/assignment/beta_u2')\n",
    "bu=bu.set_index('userID');bu=bu.bu\n",
    "for (i,u) in training1.index:\n",
    "    training1.at[(i,u),'std']=userinfo.at[u,'std']\n",
    "    training1.at[(i,u),'mean']=userinfo.at[u,'mean']\n",
    "    training1.at[(i,u),'count']=userinfo1.at[u,'count']\n",
    "    training1.at[(i,u),'std1']=userinfo1.at[u,'std']\n",
    "    training1.at[(i,u),'mean1']=userinfo1.at[u,'median']\n",
    "    training1.at[(i,u),'std2']=userinfo2.at[u,'std']\n",
    "    training1.at[(i,u),'mean2']=userinfo2.at[u,'median']\n",
    "    training1.at[(i,u),'std3']=userinfo3.at[u,'std']\n",
    "    training1.at[(i,u),'mean3']=userinfo3.at[u,'median']\n",
    "    training1.at[(i,u),'bu']=bu[u]\n",
    "    training1.at[(i,u),'med']=userinfo.at[u,'median']\n",
    "    training1.at[(i,u),'item']=iteminfo.at[i,'mean']\n",
    "training1['mean22']=training1['mean']**2\n",
    "training1['mean24']=training1['mean22']**2\n",
    "training1['item2']=training1['item']**2\n",
    "training1['item4']=training1['item2']**2\n",
    "'''\n",
    "training1['mean25']=training1['mean24']*training1['mean']\n",
    "training1['med22']=training1['med']**2\n",
    "training1['med24']=training1['med22']**2\n",
    "training1['std22']=training1['std2']**2\n",
    "training1['std12']=training1['std1']**2\n",
    "training1['rating2']=training1['rating']**2\n",
    "'''\n",
    "'''\n",
    "for (i,u) in training1.index:\n",
    "    if training1.at[(i,u),'reviewlen']<0:\n",
    "        training1.at[(i,u),'reviewlen']=1\n",
    "'''\n",
    "training1['one']=1\n",
    "train_list=[]\n",
    "training1['logoutof']=np.log(training1.outof)\n",
    "for k in xrange(1,51):\n",
    "    train_list.append(training1[training1.outof==k])\n",
    "#while k<50:\n",
    "#    train_list.append(training1[training1.outof>k][training1.outof<=k+10])\n",
    "#    k+=10\n",
    "while k<150:\n",
    "    train_list.append(training1[training1.outof>k][training1.outof<=k+30])\n",
    "    k+=30\n",
    "print k\n",
    "train_list.append(training1[training1.outof>k][training1.outof<300])\n",
    "train_list.append(training1[training1.outof>=300][training1.outof<500])\n",
    "train_list.append(training1[training1.outof>=500][training1.outof<2500])\n",
    "n=len(train_list)\n",
    "\n",
    "helpful=pd.read_csv('/Users/limeng/Documents/workspace/CSE_255/Assignment1/helpful_info.csv')\n",
    "helpful=helpful.set_index(['itemID','reviewerID'])\n",
    "helpful1=helpful[helpful.outOf>0]\n",
    "helpful0=helpful[helpful.outOf==0]\n",
    "for (i,u) in helpful1.index:\n",
    "    if u in userinfo.index:\n",
    "        helpful1.at[(i,u),'std']=userinfo.at[u,'std']\n",
    "        helpful1.at[(i,u),'mean']=userinfo.at[u,'mean']\n",
    "        helpful1.at[(i,u),'count']=userinfo1.at[u,'count']\n",
    "        helpful1.at[(i,u),'std1']=userinfo1.at[u,'std']\n",
    "        helpful1.at[(i,u),'mean1']=userinfo1.at[u,'median']\n",
    "        helpful1.at[(i,u),'std2']=userinfo2.at[u,'std']\n",
    "        helpful1.at[(i,u),'mean2']=userinfo2.at[u,'median']\n",
    "        helpful1.at[(i,u),'std3']=userinfo3.at[u,'std']\n",
    "        helpful1.at[(i,u),'mean3']=userinfo3.at[u,'median']\n",
    "        helpful1.at[(i,u),'bu']=bu[u]\n",
    "        helpful1.at[(i,u),'med']=userinfo.at[u,'mean']\n",
    "    elif u not in userinfo.index:\n",
    "        helpful1.at[(i,u),'std']=0.299999\n",
    "        helpful1.at[(i,u),'mean']=0.723289\n",
    "        helpful1.at[(i,u),'count']=27.982986#14.979567\n",
    "        helpful1.at[(i,u),'std1']=7.709194#6.798623#7.709194\n",
    "        helpful1.at[(i,u),'mean1']=3.340911#5.101805\n",
    "        helpful1.at[(i,u),'std2']=0.810503#0.790877#0.810503\n",
    "        helpful1.at[(i,u),'mean2']=4.231367#4.111923\n",
    "        helpful1.at[(i,u),'std3']=81.420824#80.475449#81.420824\n",
    "        helpful1.at[(i,u),'mean3']=167.827591#177.021279\n",
    "        helpful1.at[(i,u),'bu']=2.9667608997176091e-07\n",
    "        helpful1.at[(i,u),'std4']=0.299995\n",
    "        helpful1.at[(i,u),'mean4']=0.723287\n",
    "        helpful1.at[(i,u),'med']=0.810874\n",
    "    if i in iteminfo.index:\n",
    "        helpful1.at[(i,u),'item']=iteminfo.at[i,'mean']\n",
    "    else:\n",
    "        helpful1.at[(i,u),'item']=0.833808#6.130143#0.833808\n",
    "helpful1['mean22']=helpful1['mean']**2\n",
    "helpful1['mean24']=helpful1['mean22']**2\n",
    "#helpful1['mean25']=helpful1['mean24']*helpful1['mean']\n",
    "'''\n",
    "helpful1['med22']=helpful1['med']**2\n",
    "helpful1['med24']=helpful1['med22']**2\n",
    "helpful1['std22']=helpful1['std2']**2\n",
    "helpful1['std12']=helpful1['std1']**2\n",
    "helpful1['rating2']=helpful1['rating']**2\n",
    "'''\n",
    "helpful1['one']=1\n",
    "helpful0['pred']=0\n",
    "helpful1['logoutof']=np.log(helpful.outOf)\n",
    "helpful1['item2']=helpful1['item']**2\n",
    "helpful1['item4']=helpful1['item2']**2\n",
    "#helpful1['reviewlen']=helpful1.reviewText.apply(lambda x:np.log(len(str(x).split())))\n",
    "helpful1['reviewlen']=helpful1.reviewText.apply(lambda x:len(str(x).split()))\n",
    "print 'train model'\n",
    "#z=10000\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "line_list=[None]*n\n",
    "#pca_train=[None]*n\n",
    "index_name=list('abcdefg')\n",
    "for k in xrange(n):\n",
    "    poly=PolynomialFeatures(2)\n",
    "    #pca_train[k]=PCA(n_components=7)\n",
    "    #X=train_list[k][['rating','logoutof','mean22','mean24'\n",
    "    #    ,'std1','std','std2','mean','bu','std4','med']]\n",
    "    X=train_list[k][['rating','logoutof','mean','item','one','std1']]\n",
    "    Y=poly.fit_transform(X)\n",
    "    #pca_train[k].fit(X)\n",
    "    #Y= pd.DataFrame(np.matrix(pca_train[k].components_)*(np.matrix(X.T)),\n",
    "                    #index=index_name,columns=X.index).T\n",
    "    line_list[k]=LinearRegression(fit_intercept=False)\n",
    "    line_list[k].fit(Y,train_list[k].alpha)\n",
    "    #line_list[k].fit(train_list[k][['reviewlen','rating','logoutof','mean22','mean24'\n",
    "    #    ,'std1','std','std2','mean','bu']],train_list[k].alpha)\n",
    "\n",
    "print 'predicting'\n",
    "\n",
    "help_list=[]\n",
    "for k in xrange(1,51):\n",
    "    help_list.append(helpful1[helpful1.outOf==k])\n",
    "#while k<50:\n",
    "#    help_list.append(helpful1[helpful1.outOf>k][helpful1.outOf<=k+10])\n",
    "#    k+=10\n",
    "while k<150:\n",
    "    help_list.append(helpful1[helpful1.outOf>k][helpful1.outOf<=k+30])\n",
    "    k+=30\n",
    "print 'k',k\n",
    "help_list.append(helpful1[helpful1.outOf>k][helpful1.outOf<300])\n",
    "help_list.append(helpful1[helpful1.outOf>=300][helpful1.outOf<500])\n",
    "help_list.append(helpful1[helpful1.outOf>=500][helpful1.outOf<2500])\n",
    "for x in xrange(n):\n",
    "    #X=help_list[x][['rating','logoutof','mean22','mean24','std1','std','std2','mean','bu','std4','med']]\n",
    "    X=help_list[x][['rating','logoutof','mean','item','one','std1']]\n",
    "    poly=PolynomialFeatures(2)\n",
    "    Y=poly.fit_transform(X)\n",
    "    #Y= pd.DataFrame(np.matrix(pca_train[x].components_)*(np.matrix(X.T)),\n",
    "                    #index=index_name,columns=X.index).T\n",
    "    help_list[x]['pred']=line_list[x].predict(Y)\n",
    "    #help_list[x]['pred']=line_list[x].predict(help_list[x][['reviewlen','rating','logoutof','mean22','mean24'\n",
    "    #    ,'std1','std','std2','mean','bu']])\n",
    "\n",
    "helpful=pd.concat(help_list)\n",
    "helpful=pd.concat([helpful,helpful0])\n",
    "print 'start'\n",
    "predictions = open(\"/Users/limeng/Documents/workspace/CSE_255/week_05/predictions_Helpful10.txt\", 'w')\n",
    "for l in open(\"/Users/limeng/Documents/workspace/CSE_255/week_05/pairs_Helpful1.txt\"):\n",
    "  if l.startswith(\"userID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i,outOf = l.strip().split('-')\n",
    "  outOf = int(outOf)\n",
    "  predictions.write(u + '-' + i + '-' + str(outOf) + ',' +\n",
    "                    str(roundi(outOf*(helpful.at[(i,u),'pred']),0.4)) + '\\n')\n",
    "predictions.close()\n",
    "print 'done'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
