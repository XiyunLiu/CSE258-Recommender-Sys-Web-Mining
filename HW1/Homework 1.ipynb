{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "from math import exp\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "To solve this equation $review\\,/\\,overall ≃ \\theta_0 + \\theta_1 × year$ , I use python library numpy.linalg.lstsq\n",
    "\n",
    "The fitted values are: $\\theta_0 = -3.91707489*10^{-1}, \\theta_1 = 2.14379786*10^{-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "winedata = list(parseData(\"http://jmcauley.ucsd.edu/cse255/data/beer/beer_50000.json\"))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.91707489e+01   2.14379786e-02] [ 0.49004382]\n"
     ]
    }
   ],
   "source": [
    "year = np.array([review['review/timeStruct']['year'] for review in winedata])\n",
    "y = np.array([review['review/overall'] for review in winedata])\n",
    "X = np.vstack([np.ones(len(year)), year]).T\n",
    "theta, residuals, rank, s = np.linalg.lstsq(X, y)\n",
    "MSE = residuals/len(year)\n",
    "print theta, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "I add a feature which is the square of year since I am considering that the overall review may not be linear with year. $review\\,/\\,overall ≃ \\theta_0 + \\theta_1 × year+\\theta_2 × year^2$\n",
    "\n",
    "The Mean Square Error is 0.49004382 in problem 1, and now it improves to 0.49004374. The improvement is quite small. Actually I think year is not a very good feature to predict the overall review and the review is not very independent on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.32111616e+02   2.13653191e-01  -4.78729979e-05] [ 0.49004374]\n"
     ]
    }
   ],
   "source": [
    "X_2 = np.vstack([np.ones(len(year)), year, year**2]).T\n",
    "theta_2,residuals_2, rank_2, s_2 = np.linalg.lstsq(X_2, y)\n",
    "MSE_2 = residuals_2/len(year)\n",
    "print theta_2, MSE_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "The fitted coefficients on training are\n",
    "\n",
    "[  2.56420279e+02,   1.35421303e-01,  -1.72994866e+00,\n",
    "         1.02651152e-01,   1.09038568e-01,  -2.76775146e-01,\n",
    "         6.34332168e-03,   3.85023977e-05,  -2.58652809e+02,\n",
    "         1.19540566e+00,   8.33006285e-01,   9.79304353e-02]\n",
    "         \n",
    "The MSE on the training set is 0.602307502903, the MSE on the test set is 0.562457130315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('winequality-white.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';')\n",
    "    content = []\n",
    "    for row in reader:\n",
    "        content.append(row)\n",
    "indexLabel = {}\n",
    "for index, label in enumerate(content[0]):\n",
    "    indexLabel[index+1] = label\n",
    "data = map(lambda row: [1]+[float(x) for x in row], content[1:])\n",
    "trainingSet = np.array(data[:len(data)/2])\n",
    "testSet = np.array(data[len(data)/2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_wine, residuals_wine, rank_wine, s_wine = np.linalg.lstsq(trainingSet[:,:-1], trainingSet[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.56420279e+02,   1.35421303e-01,  -1.72994866e+00,\n",
       "         1.02651152e-01,   1.09038568e-01,  -2.76775146e-01,\n",
       "         6.34332168e-03,   3.85023977e-05,  -2.58652809e+02,\n",
       "         1.19540566e+00,   8.33006285e-01,   9.79304353e-02])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.602307502903\n",
      "0.562457130315\n"
     ]
    }
   ],
   "source": [
    "print mean_squared_error(np.dot(trainingSet[:,:-1],theta_wine), trainingSet[:,-1])\n",
    "print mean_squared_error(np.dot(testSet[:,:-1],theta_wine), testSet[:,-1])\n",
    "mse_all = mean_squared_error(np.dot(testSet[:,:-1],theta_wine), testSet[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "The MSEs on the test set of all 11 ablation experiments are shown under the code below.\n",
    "\n",
    "Based on the test MSEs, volatile acidity provides the most additional information since (MSE[excluding volatile acidity] - MSE[all feature]) is the maximum one.\n",
    "\n",
    "The feature \"density\" provides the least additional information beyond what is present in the 10 other features since (MSE[excluding density] - MSE[including density]) is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove fixed acidity , the MSE on test is 0.559113414376\n",
      "Remove volatile acidity , the MSE on test is 0.596384850161\n",
      "Remove citric acid , the MSE on test is 0.562221702812\n",
      "Remove residual sugar , the MSE on test is 0.553625063967\n",
      "Remove chlorides , the MSE on test is 0.562629266481\n",
      "Remove free sulfur dioxide , the MSE on test is 0.55614081793\n",
      "Remove total sulfur dioxide , the MSE on test is 0.562429005469\n",
      "Remove density , the MSE on test is 0.544726553466\n",
      "Remove pH , the MSE on test is 0.559566626382\n",
      "Remove sulphates , the MSE on test is 0.557346349988\n",
      "Remove alcohol , the MSE on test is 0.573214743558\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(1,12):\n",
    "    train, test = np.delete(trainingSet, i, axis=1), np.delete(testSet, i, axis=1)\n",
    "    theta_ablation, residuals_ablation, rank_ablation, s_ablation = \\\n",
    "    np.linalg.lstsq(train[:,:-1], train[:,-1])\n",
    "    MSE = mean_squared_error(np.dot(test[:,:-1],theta_ablation), test[:,-1])\n",
    "    print \"Remove\", indexLabel[i],\", the MSE on test is\", MSE\n",
    "    result.append([indexLabel[i],MSE])\n",
    "# result.sort(key = lambda x: x[1])\n",
    "# for re in result:\n",
    "#     if re[1] > mse_all:\n",
    "#         print re[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "Here I use the library sklearn.svm.SVC to train the data. Based on experiments, I found C=0.5 works relatively well.\n",
    "\n",
    "The accuracy (percentage of correct classifications) of the predictor is 84.8509% on the train data, and 69.90608% on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData, trainingLabel = trainingSet[:,:-1], trainingSet[:,-1]\n",
    "trainingLabel = map(lambda x: 1.0 if x>5 else 0.0, trainingLabel)\n",
    "testData, testLabel = testSet[:,:-1], testSet[:,-1]\n",
    "testLabel = map(lambda x: 1 if x>5 else 0, testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=0.5)\n",
    "clf.fit(trainingData, trainingLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 0.844426296448\n",
      "test 0.69906084116\n"
     ]
    }
   ],
   "source": [
    "correct = filter(lambda x: x[0] == x[1], zip(clf.predict(trainingData),trainingLabel))\n",
    "correctRate = len(correct)*1.0/len(trainingLabel)\n",
    "print \"training:\", correctRate\n",
    "correct_test = filter(lambda x: x[0] == x[1], zip(clf.predict(testData), testLabel))\n",
    "correctRate_test = len(correct_test)*1.0/len(testLabel)\n",
    "print \"test\", correctRate_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Problem 6\n",
    "The final log-likelihood after convergence is -1383.18949552, and the accuracy of the resulting model on the test set is 0.767251939567\n",
    "\n",
    "Noted that first, the bias is added in the training data. Second, the final negative log-likelihood given by scipy.optimize.fmin_l_bfgs_b includes the regularization since the function f which is passed into this library includes the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.09893691e+00  -2.59661194e-01  -4.26334221e+00   2.22926668e-01\n",
      "   1.47520329e-02  -5.41120346e-01   1.59255350e-02  -1.77788493e-03\n",
      "  -2.09625819e+00  -4.45081694e-01   1.18692510e+00   8.05788118e-01]\n",
      "Final log likelihood =  -1383.18949552\n"
     ]
    }
   ],
   "source": [
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    return -loglikelihood\n",
    "\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = np.array([0.0]*len(theta))\n",
    "    for i in range(len(X)):\n",
    "        dl += (np.array(X[i])/(1+exp(np.dot(theta, np.array(X[i])))))\n",
    "        if not y[i]:\n",
    "            dl -= X[i]\n",
    "    dl -= 2*lam*np.array(theta) \n",
    "    # Negate the return value since we're doing grad ient *ascent*\n",
    "    return np.array([-x for x in dl])\n",
    "\n",
    "# Use a library function to run gradient descent (or you can implement yourself!)\n",
    "result = scipy.optimize.fmin_l_bfgs_b(f, np.zeros(len(trainingData[0])), fprime, args = (trainingData, trainingLabel, 1))\n",
    "theta = result[0]\n",
    "print theta\n",
    "print \"Final log likelihood = \", -result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.767251939567\n"
     ]
    }
   ],
   "source": [
    "probability = sigmoid(np.dot(testData, theta))\n",
    "predict = map(lambda prob: 1 if prob >= 0.5 else 0, probability)\n",
    "print \"Accuracy =\",len(filter(lambda x: x[0] == x[1], zip(predict, testLabel)))*1.0/len(testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
