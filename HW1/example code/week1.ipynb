{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse255/data/beer/beer_50000.json\"))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.linalg.lstsq(a, b) : Solves the equation a x = b by computing a vector x that minimizes the Euclidean 2-norm || b - a x ||^2.  \n",
    "\n",
    "#### a: “Coefficient” matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y) # no offset, theta = 3.88871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 3.88871]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Convince ourselves that basic linear algebra operations yield the same answer ###\n",
    "\n",
    "X = numpy.matrix(X)\n",
    "y = numpy.matrix(y)\n",
    "numpy.linalg.inv(X.T * X) * X.T * y.T # same as theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Do older people rate beer more highly? ###\n",
    "\n",
    "data2 = [d for d in data if d.has_key('user/ageInSeconds')]\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  #feat.append(datum['user/ageInSeconds'])\n",
    "  feat.append(datum['beer/ABV'])\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data2]\n",
    "y = [d['review/overall'] for d in data2]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### How much do women prefer beer over men? ###\n",
    "\n",
    "data2 = [d for d in data if d.has_key('user/gender')]\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  if datum['user/gender'] == \"Male\":\n",
    "    feat.append(0)\n",
    "  else:\n",
    "    feat.append(1)\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data2]\n",
    "y = [d['review/overall'] for d in data2]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function = $\\frac{1}{N}||y-X\\theta||_2^2+\\lambda ||\\theta||^2_2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[15.730542077145518]]\n",
      "gradient = [-7.8098809  -0.12091359]\n",
      "offset = [[9.01995653088751]]\n",
      "gradient = [-5.60967714 -0.08716214]\n",
      "offset = [[1.8678065242136217]]\n",
      "gradient = [  1.53716338e-05  -9.92863150e-04]\n",
      "offset = [[1.8678060995104309]]\n",
      "gradient = [  1.39425606e-05  -8.89205217e-04]\n",
      "offset = [[1.8678043781773443]]\n",
      "gradient = [  1.03361764e-12   1.18134669e-15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.54912554,  0.05977487]),\n",
       " [1.8678043781773443],\n",
       " {'funcalls': 5,\n",
       "  'grad': array([  1.03361764e-12,   1.18134669e-15]),\n",
       "  'nit': 4,\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Gradient descent ###\n",
    "\n",
    "# Objective\n",
    "def f(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  diffSq = diff.T*diff\n",
    "  diffSqReg = diffSq / len(X) + lam*(theta.T*theta)\n",
    "  print \"offset =\", diffSqReg.flatten().tolist()\n",
    "  return diffSqReg.flatten().tolist()[0]\n",
    "\n",
    "# Derivative\n",
    "def fprime(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  res = 2*X.T*diff / len(X) + 2*lam*theta\n",
    "  print \"gradient =\", numpy.array(res.flatten().tolist()[0])\n",
    "  return numpy.array(res.flatten().tolist()[0])\n",
    "\n",
    "scipy.optimize.fmin_l_bfgs_b(f, [0,0], fprime, args = (X, y, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random features ###\n",
    "\n",
    "def feature(datum):\n",
    "  return [random.random() for x in range(30)]\n",
    "\n",
    "X = [feature(d) for d in data2]\n",
    "y = [d['review/overall'] for d in data2]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "\n",
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = Read in the data\n",
    "print \"done\"\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  print \"ll =\", loglikelihood\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0.0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    # Fill in code for the derivative\n",
    "    pass\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "  return numpy.array([-x for x in dl])\n",
    "\n",
    "\n",
    "X = # Extract features and labels from the data\n",
    "y = \n",
    "\n",
    "X_train = X[:len(X)/2]\n",
    "X_test = X[3*len(X)/4:]\n",
    "\n",
    "# Use a library function to run gradient descent (or you can implement yourself!)\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, y_train, 1.0))\n",
    "print \"Final log likelihood =\", -l\n",
    "\n",
    "print \"Accuracy = \" # Compute the accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
